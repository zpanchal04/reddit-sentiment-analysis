## Next-Gen Reddit Sentiment AI Dashboard

An end-to-end sentiment analysis project for Reddit data with a modern interactive web dashboard. It collects Reddit posts, cleans and labels text, trains multiple ML models, and visualizes insights via a Streamlit + Plotly app. Includes reproducible notebooks, tests, and saved model artifacts.

### Key Features
- **Data collection**: Incremental fetch via Reddit API (PRAW) with duplicate avoidance and persistent JSON storage
- **Preprocessing**: Robust text cleaning, tokenization helpers
- **Rule-based sentiment**: VADER sentiment labeling utility
- **ML modeling**: Pipelines with TF-IDF + classifiers (Logistic Regression, SVM, Naive Bayes, Random Forest, Gradient Boosting)
- **Interactive dashboard**: Streamlit app with Plotly interactive 3D charts, white headings and unified black chart background
- **Model training UI**: Train and compare models inside the app, export results
- **Comment analyzer**: Real-time sentiment on all comments for any Reddit post URL
- **Reproducible notebooks**: Data collection, preprocessing, modeling, and visualization notebooks

### Project Structure (important files)
```
dashboard.py                   # Streamlit dashboard (interactive app)
requirements.txt               # Python dependencies
src/                           # helper modules
  ai_helper.py                  # Give AI prompt 
  analyzer.py                   # VADER utilities and ML pipeline helper
  preprocessor.py               # Text cleaning and tokenization helpers
  fetch_data.py                 # Reddit fetcher (incremental, PRAW)
data/
  raw/reddit-posts.json         # Raw Reddit posts
  processed/cleaned-posts.csv   # Cleaned posts
  processed/labeled-posts.csv   # Labeled posts (with sentiment)
models/                         # Saved model pipelines/artifacts (joblib)
notebook/                       # Jupyter notebooks: 01..04
outputs/                        # Exported visuals and reports (PNG/PDF)
tests/                          # Simple unit tests
```

### Quick setup
1. Python 3.9+ recommended (3.10/3.11 tested)

2. Create and activate a virtual environment
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1   # PowerShell (Windows)
# or use .\.venv\Scripts\activate for cmd.exe
```

3. Install dependencies
```powershell
pip install -r requirements.txt
```

4. Add secrets and API keys in `.env` (do NOT commit)
- Reddit / PRAW credentials (if you run data collection):
  CLIENT_ID, CLIENT_SECRET, USERNAME, PASSWORD, USER_AGENT
- Gemini (required for AI model comparison):
  GEMINI_API_KEY=your_real_gemini_api_key_here
  (Optional) GEMINI_MODEL=gemini-1.5   # override model name used by the dashboard

Tip: You can set the key for the current PowerShell session with:
```powershell
$env:GEMINI_API_KEY = 'your_real_gemini_api_key_here'
```
Or persist it for your user with:
```powershell
setx GEMINI_API_KEY "your_real_gemini_api_key_here"
```

### Running the dashboard
```powershell
streamlit run .\dashboard.py
```
Main capabilities:
- KPIs, sentiment distribution, time trends, top subreddits
- Model Training & Evaluation (train inside the UI and save models to `models/`)
- Sample Data export: sidebar supports selecting a format and downloading `data/processed/sample_data.csv` converted into the chosen format (including an SQL `.sql` export that contains CREATE TABLE + INSERT statements)
- AI Model Comparison: produces a theoretical comparison table generated by Google Gemini and displays/downloads it in Tab 2 (requires valid `GEMINI_API_KEY`)

### Sample data export
- The sidebar includes a `Sample Data` section. Supported formats include CSV, JSON, Excel (.xlsx), Parquet (.parquet), Feather, HTML, XML, HDF5 (.h5), Pickle (.pkl) and SQL (.sql).
- The app converts `data/processed/sample_data.csv` on-the-fly and offers a download button.
- Excel, Parquet, Feather, HDF5 and XML exports require optional dependencies (`openpyxl`, `pyarrow`, `tables`, `lxml`). If missing, the app shows a helpful message.

### AI-powered theoretical model comparison (Gemini)
- The dashboard uses the installed `google.generativeai` SDK and calls Gemini via `genai.GenerativeModel(...).generate_content(...)`.
- The comparison is produced only via Gemini (no local fallbacks); the dashboard expects the model to return valid JSON. If Gemini fails (invalid key, quota, or malformed output) the app will show a clear error message.

Quick verification script
Create a small `verify_gemini_key.py` (or run the snippet below) to validate your key before using the dashboard:
```python
import os
import google.generativeai as genai

key = os.getenv('GEMINI_API_KEY')
print('Has key:', bool(key))
genai.configure(api_key=key)
model = genai.GenerativeModel(os.getenv('GEMINI_MODEL', 'gemini-1.5'))
resp = model.generate_content('Return a one-line JSON: {"ok": true}')
print(getattr(resp, 'text', getattr(resp, 'candidates', None)))
```

Run it in PowerShell:
```powershell
python .\verify_gemini_key.py
```

If you receive a 400 API_KEY_INVALID error, see the Troubleshooting section below.

### Testing & Notebooks
- Notebooks live in `notebook/` and cover data collection, preprocessing, modeling and visualization.
- Run unit tests with `pytest -q`.

### Troubleshooting GEMINI API key (common causes)
- 400 API_KEY_INVALID: The key in `.env` or environment variables is invalid/expired or belongs to a project where the Generative Language API is not enabled. Remedy:
  1. Create or rotate an API key in Google Cloud Console -> APIs & Services -> Credentials.
  2. Ensure the Generative Language (Generative AI) API is enabled for that GCP project.
  3. Update `.env` or set the env var in PowerShell (see examples above). Restart the terminal/IDE if you used `setx`.
  4. If you use API key restrictions (IP/HTTP referrers), create an unrestricted key for local testing or allow your local environment.

### Troubleshooting other issues
- Missing optional format libraries: install the packages named in `requirements.txt` (openpyxl, pyarrow, tables, lxml) if you need the corresponding export formats.
- PRAW auth errors: verify `.env` fields and app credentials.

### Roadmap
- Add Transformer-based sentiment models and a scheduled ingestion pipeline
- Dockerize the app and add CI for tests and model artifact publishing

### Security & Privacy

- Do not commit `.env` with secrets

